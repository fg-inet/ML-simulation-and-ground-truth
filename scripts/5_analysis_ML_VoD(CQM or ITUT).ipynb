{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.stats import variation\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LassoCV,Lasso,Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from skorch import NeuralNetRegressor\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "import torch.utils.data as Data\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import json\n",
    "from termcolor import colored\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "# import pickle\n",
    "import joblib\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score  \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,Normalizer\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression,mutual_info_regression\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "#Evaluation\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "warnings.filterwarnings('ignore', 'Objective did not converge.*')\n",
    "\n",
    "#Avoiding Type 3 fonts in matplotlib plots\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'size'   : 40}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('lines', linewidth=3.0)\n",
    "matplotlib.rc('lines', markersize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13952, 96)\n"
     ]
    }
   ],
   "source": [
    "#read the metrics and target values from local files\n",
    "# mat=pd.read_pickle('QoE_metrics_VoD(CQM_ITUT)_13952.pkl')\n",
    "#if the metrics is csv file\n",
    "mat=pd.read_csv('QoE_metrics_VoD(CQM_ITUT)_13952.csv',index_col=0)\n",
    "\n",
    "#for training and testing CQM\n",
    "mat=mat.rename(columns={'QoE_CQM':'QoE'})\n",
    "mark='CQM'\n",
    "#for training and testing ITUT\n",
    "# mat=mat.rename(columns={'QoE_ITUT':'QoE'})\n",
    "# mark=ITUT\n",
    "\n",
    "mat=mat.fillna(0)\n",
    "X=mat.iloc[:,-(mat.shape[1]-2):]\n",
    "\n",
    "Y=mat['QoE']\n",
    "#drop columns which have same values in all rows\n",
    "# X=X.drop(X.std()[(X.std() == 0)].index, axis=1)\n",
    "maxnum_feature=X.shape[1]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isinf(mat).sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into different speed of movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2791, 96), (630, 96), (2161, 96), (1037, 96), (1124, 96))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2,random_state=17)\n",
    "\n",
    "X_stationary=X_test.filter(like='_nm_', axis=0)\n",
    "Y_stationary=Y_test.filter(like='_nm_', axis=0)\n",
    "\n",
    "X_moving=X_test.drop(X_test.filter(like='_nm_', axis=0).index)\n",
    "Y_moving=Y_test.drop(Y_test.filter(like='_nm_', axis=0).index)\n",
    "\n",
    "X_pedestrians=X_moving.filter(regex='_s50-50_80_cli([0123]?[0-9]_)|_s100-0_|_s50-50_160_cli([01234567]?[0-9]_)',axis=0)\n",
    "Y_pedestrians=Y_moving.filter(regex='_s50-50_80_cli([0123]?[0-9]_)|_s100-0_|_s50-50_160_cli([01234567]?[0-9]_)',axis=0)\n",
    "\n",
    "X_vehicles=X_moving.filter(regex='_s50-50_80_cli(4[0-9]|[567][0-9]_)|_s0-100_|_s50-50_160_cli([89][0-9]|1[0-5][0-9])',axis=0)\n",
    "Y_vehicles=Y_moving.filter(regex='_s50-50_80_cli(4[0-9]|[567][0-9]_)|_s0-100_|_s50-50_160_cli([89][0-9]|1[0-5][0-9])',axis=0)\n",
    "\n",
    "X_test.shape,X_stationary.shape,X_moving.shape,X_pedestrians.shape,X_vehicles.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "Linear Model trained with L1 prior as regularizer (aka the Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:   22.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CPUs utilized percentage : 50.7 %\n",
      "the duration of training is: 23.770852088928223\n",
      "Percentage of used RAM : 64.0 %\n"
     ]
    }
   ],
   "source": [
    "pipe_Lasso=Pipeline([('scale', StandardScaler()),('clf', Lasso())])\n",
    "# alphas=[1e-9, 0.0001, 0.0003, 0.0005,0.00055,0.0006,0.00065,0.0007, 0.0008,0.0009,0.001, 0.003, 0.005, 0.007, 0.01, 0.03, 0.05, 0.07, 0.08, 0.09, 0.1]\n",
    "alphas=[0.0001,0.0002,0.0003,0.0005,0.0007,0.001,0.005,0.01, 0.03, 0.05, 0.07, 0.08, 0.09, 0.1]\n",
    "Lasso_params={'clf__alpha': alphas}\n",
    "cv_Lasso = GridSearchCV(pipe_Lasso, param_grid=Lasso_params, cv=5, scoring='neg_mean_squared_error',n_jobs=-1, verbose=3,return_train_score=True)\n",
    "start=time.time()\n",
    "cv_Lasso.fit(X_train, Y_train)\n",
    "#save the model\n",
    "joblib.dump(cv_Lasso,'model_%s_Lasso_MSE.joblib'%mark)\n",
    "\n",
    "vcpu=psutil.cpu_percent()\n",
    "print ('Total CPUs utilized percentage :',vcpu,'%')\n",
    "finish=time.time()\n",
    "print('the duration of training is:',finish-start)\n",
    "\n",
    "print('Percentage of used RAM :',psutil.virtual_memory().percent,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:  0.06698094205764775\n",
      "stationary:  0.08492378969654446\n",
      "moving:  0.061750033213360415\n",
      "pedestrians:  0.05435027170676903\n",
      "vehicles:  0.06857703737913913\n"
     ]
    }
   ],
   "source": [
    "#Get the MSE score for different type in the test dataset by applying the same model\n",
    "print('--------------Lasso------------')\n",
    "print('All: ',mean_squared_error(Y_test,cv_Lasso.predict(X_test)))\n",
    "print('stationary: ',mean_squared_error(Y_stationary,cv_Lasso.predict(X_stationary)))\n",
    "print('moving: ',mean_squared_error(Y_moving,cv_Lasso.predict(X_moving)))\n",
    "print('pedestrians: ',mean_squared_error(Y_pedestrians,cv_Lasso.predict(X_pedestrians)))\n",
    "print('vehicles: ',mean_squared_error(Y_vehicles,cv_Lasso.predict(X_vehicles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation score: -0.06912046216715706\n",
      "Best parameters: {'clf__alpha': 0.0003}\n",
      "Test score:  0.06698094205764775\n",
      "Total CPUs utilized percentage : 4.7 %\n",
      "the duration of test is: 0.00956583023071289\n",
      "Percentage of used RAM : 64.2 %\n",
      "The training score of the best estimator:  -0.06767183632770972\n"
     ]
    }
   ],
   "source": [
    "print('Best validation score: {}'.format(cv_Lasso.best_score_))\n",
    "print('Best parameters: {}'.format(cv_Lasso.best_params_))\n",
    "start=time.time()\n",
    "print('Test score: ',mean_squared_error(Y_test,cv_Lasso.predict(X_test)))\n",
    "vcpu=psutil.cpu_percent()\n",
    "print ('Total CPUs utilized percentage :',vcpu,'%')\n",
    "finish=time.time()\n",
    "print('the duration of test is:',finish-start)\n",
    "print('Percentage of used RAM :',psutil.virtual_memory().percent,'%')\n",
    "\n",
    "train_scores_mean = cv_Lasso.cv_results_[\"mean_train_score\"]\n",
    "test_scores_mean = cv_Lasso.cv_results_[\"mean_test_score\"]\n",
    "print('The training score of the best estimator: ',train_scores_mean[np.argmax(test_scores_mean)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Ridge Regression \n",
    "Linear least squares with l2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    2.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CPUs utilized percentage : 36.9 %\n",
      "the duration of training is: 4.0333569049835205\n",
      "Percentage of used RAM : 66.7 %\n",
      "CPU times: user 1.01 s, sys: 93.8 ms, total: 1.11 s\n",
      "Wall time: 4.05 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  55 out of  55 | elapsed:    3.8s finished\n"
     ]
    }
   ],
   "source": [
    "pipe_LRR = Pipeline([('scale', StandardScaler()),('clf', Ridge())])\n",
    "\n",
    "# alphas=[1e-9,1e-8,1e-7,1e-6,1e-5,1e-4, 0.0003, 0.0005, 0.0007, 0.001, 0.003, 0.005, 0.007, 0.01, 0.03, 0.05, 0.07, 0.08, 0.09, 0.1, 0.3, 0.5, 0.7,1,5,7,10,100,500] \n",
    "alphas=[1e-4, 0.0003, 0.0005, 0.0007,0.001, 0.003, 0.005, 0.007,0.01,0.1,70] \n",
    "\n",
    "LRR_params ={'clf__alpha': alphas}\n",
    "\n",
    "cv_LRR = GridSearchCV(pipe_LRR, param_grid=LRR_params, cv=5, scoring='neg_mean_squared_error',n_jobs=-1, verbose=3,return_train_score=True)\n",
    "start=time.time()\n",
    "cv_LRR.fit(X_train, Y_train)\n",
    "#save the model\n",
    "joblib.dump(cv_LRR,'model_%s_LRR_MSE.joblib'%mark)\n",
    "\n",
    "vcpu=psutil.cpu_percent()\n",
    "print ('Total CPUs utilized percentage :',vcpu,'%')\n",
    "finish=time.time()\n",
    "print('the duration of training is:',finish-start)\n",
    "\n",
    "print('Percentage of used RAM :',psutil.virtual_memory().percent,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:  0.06312404837660893\n",
      "stationary:  0.0789270533337478\n",
      "moving:  0.058516971503403244\n",
      "pedestrians:  0.05161448534543975\n",
      "vehicles:  0.06488519049433575\n"
     ]
    }
   ],
   "source": [
    "#Get the MSE score for different type in the test dataset by applying the same model\n",
    "print('--------------LRR------------')\n",
    "print('All: ',mean_squared_error(Y_test,cv_LRR.predict(X_test)))\n",
    "print('stationary: ',mean_squared_error(Y_stationary,cv_LRR.predict(X_stationary)))\n",
    "print('moving: ',mean_squared_error(Y_moving,cv_LRR.predict(X_moving)))\n",
    "print('pedestrians: ',mean_squared_error(Y_pedestrians,cv_LRR.predict(X_pedestrians)))\n",
    "print('vehicles: ',mean_squared_error(Y_vehicles,cv_LRR.predict(X_vehicles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation score: -0.06558946784201304\n",
      "Best parameters: {'clf__alpha': 0.0001}\n",
      "Test score:  0.06312404837660893\n",
      "Total CPUs utilized percentage : 24.7 %\n",
      "the duration of test is: 0.007996320724487305\n",
      "Percentage of used RAM : 66.3 %\n",
      "The training score of the best estimator:  -0.06378445681296452\n"
     ]
    }
   ],
   "source": [
    "print('Best validation score: {}'.format(cv_LRR.best_score_))\n",
    "print('Best parameters: {}'.format(cv_LRR.best_params_))\n",
    "start=time.time()\n",
    "print('Test score: ',mean_squared_error(Y_test,cv_LRR.predict(X_test)))\n",
    "vcpu=psutil.cpu_percent()\n",
    "print ('Total CPUs utilized percentage :',vcpu,'%')\n",
    "finish=time.time()\n",
    "print('the duration of test is:',finish-start)\n",
    "print('Percentage of used RAM :',psutil.virtual_memory().percent,'%')\n",
    "\n",
    "train_scores_mean = cv_LRR.cv_results_[\"mean_train_score\"]\n",
    "test_scores_mean = cv_LRR.cv_results_[\"mean_test_score\"]\n",
    "print('The training score of the best estimator: ',train_scores_mean[np.argmax(test_scores_mean)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  7.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CPUs utilized percentage : 92.8 %\n",
      "the duration of test is: 472.01826095581055\n",
      "Percentage of used RAM : 28.7 %\n",
      "CPU times: user 21 s, sys: 1.75 s, total: 22.8 s\n",
      "Wall time: 7min 52s\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', KernelRidge())\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'clf__kernel': ['rbf'],\n",
    "#         'clf__alpha': [0, 0.0001, 0.0003, 0.0005, 0.0007, 0.001, 0.003, 0.005, 0.007, 0.01, 0.03, 0.05, 0.07, 0.08, 0.09, 0.1, 0.3, 0.5, 0.7, 1, 3, 5, 10]\n",
    "        'clf__alpha': [0.01, 0.03, 0.05, 0.07,0.1, 0.2,0.5, 0.7, 1,10]\n",
    "\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "#scoring=neg_mean_squared_error\n",
    "cv_krr = GridSearchCV(pipe, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error',n_jobs=-1, verbose=3,return_train_score=True)\n",
    "start=time.time()\n",
    "cv_krr.fit(X_train, Y_train)\n",
    "#save the model\n",
    "joblib.dump(cv_krr,'model_%s_krr_MSE.joblib'%mark)\n",
    "\n",
    "vcpu=psutil.cpu_percent()\n",
    "print ('Total CPUs utilized percentage :',vcpu,'%')\n",
    "finish=time.time()\n",
    "print('the duration of test is:',finish-start)\n",
    "print('Percentage of used RAM :',psutil.virtual_memory().percent,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:  0.05765112427975837\n",
      "stationary:  0.07485071559042569\n",
      "moving:  0.05263689821510291\n",
      "pedestrians:  0.04486412397369146\n",
      "vehicles:  0.05980804313355823\n"
     ]
    }
   ],
   "source": [
    "#Get the MSE score for different type in the test dataset by applying the same model\n",
    "print('--------------KRR------------')\n",
    "print('All: ',mean_squared_error(Y_test,cv_krr.predict(X_test)))\n",
    "print('stationary: ',mean_squared_error(Y_stationary,cv_krr.predict(X_stationary)))\n",
    "print('moving: ',mean_squared_error(Y_moving,cv_krr.predict(X_moving)))\n",
    "print('pedestrians: ',mean_squared_error(Y_pedestrians,cv_krr.predict(X_pedestrians)))\n",
    "print('vehicles: ',mean_squared_error(Y_vehicles,cv_krr.predict(X_vehicles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation score: -0.06443499825540126\n",
      "Best parameters: {'clf__alpha': 0.07, 'clf__kernel': 'rbf'}\n",
      "Test score:  0.05765112427975837\n",
      "Total CPUs utilized percentage : 5.8 %\n",
      "the duration of test is: 0.7514591217041016\n",
      "Percentage of used RAM : 38.1 %\n",
      "The training score of the best estimator:  -0.0339930702108747\n"
     ]
    }
   ],
   "source": [
    "print('Best validation score: {}'.format(cv_krr.best_score_))\n",
    "print('Best parameters: {}'.format(cv_krr.best_params_))\n",
    "start=time.time()\n",
    "print('Test score: ',mean_squared_error(Y_test,cv_krr.predict(X_test)))\n",
    "vcpu=psutil.cpu_percent()\n",
    "print ('Total CPUs utilized percentage :',vcpu,'%')\n",
    "finish=time.time()\n",
    "print('the duration of test is:',finish-start)\n",
    "print('Percentage of used RAM :',psutil.virtual_memory().percent,'%')\n",
    "\n",
    "train_scores_mean = cv_krr.cv_results_[\"mean_train_score\"]\n",
    "test_scores_mean = cv_krr.cv_results_[\"mean_test_score\"]\n",
    "print('The training score of the best estimator: ',train_scores_mean[np.argmax(test_scores_mean)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 23.1min\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed: 41.0min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed: 48.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CPUs utilized percentage : 95.4 %\n",
      "the duration of training is: 2933.6711165905\n",
      "Percentage of used RAM : 58.7 %\n",
      "CPU times: user 19.7 s, sys: 930 ms, total: 20.7 s\n",
      "Wall time: 48min 53s\n"
     ]
    }
   ],
   "source": [
    "pipe_SVR = Pipeline([('scale', StandardScaler()),('clf', SVR())])\n",
    "SVR_params = [\n",
    "    {\n",
    "        'clf__C': np.linspace(0.1,5, num=20),\n",
    "        'clf__gamma': [0.003,0.04,0.005,0.06,0.007,0.01],\n",
    "        'clf__kernel': ['rbf']\n",
    "    },\n",
    "]\n",
    "cv_SVR = GridSearchCV(pipe_SVR, param_grid=SVR_params, cv=5, scoring='neg_mean_squared_error',n_jobs=-1, verbose=3,return_train_score=True)\n",
    "start=time.time()\n",
    "cv_SVR.fit(X_train, Y_train)\n",
    "#save the model\n",
    "joblib.dump(cv_SVR,'model_%s_SVR_MSE.joblib'%mark)\n",
    "\n",
    "vcpu=psutil.cpu_percent()\n",
    "print ('Total CPUs utilized percentage :',vcpu,'%')\n",
    "finish=time.time()\n",
    "print('the duration of training is:',finish-start)\n",
    "print('Percentage of used RAM :',psutil.virtual_memory().percent,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:  0.05477098297547724\n",
      "stationary:  0.06335801464864224\n",
      "moving:  0.05226759104854807\n",
      "pedestrians:  0.04390564385601982\n",
      "vehicles:  0.05998230567368311\n"
     ]
    }
   ],
   "source": [
    "#Get the MSE score for different type in the test dataset by applying the same model\n",
    "print('--------------SVR------------')\n",
    "print('All: ',mean_squared_error(Y_test,cv_SVR.predict(X_test)))\n",
    "print('stationary: ',mean_squared_error(Y_stationary,cv_SVR.predict(X_stationary)))\n",
    "print('moving: ',mean_squared_error(Y_moving,cv_SVR.predict(X_moving)))\n",
    "print('pedestrians: ',mean_squared_error(Y_pedestrians,cv_SVR.predict(X_pedestrians)))\n",
    "print('vehicles: ',mean_squared_error(Y_vehicles,cv_SVR.predict(X_vehicles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation score: -0.05545148822753737\n",
      "Best parameters: {'clf__C': 5.0, 'clf__gamma': 0.005, 'clf__kernel': 'rbf'}\n",
      "Test score:  0.05477098297547724\n",
      "Total CPUs utilized percentage : 4.0 %\n",
      "the duration of test is: 1.5488080978393555\n",
      "Percentage of used RAM : 59.1 %\n",
      "The training score of the best estimator:  -0.044610899839142806\n"
     ]
    }
   ],
   "source": [
    "print('Best validation score: {}'.format(cv_SVR.best_score_))\n",
    "print('Best parameters: {}'.format(cv_SVR.best_params_))\n",
    "start=time.time()\n",
    "print('Test score: ',mean_squared_error(Y_test,cv_SVR.predict(X_test)))\n",
    "vcpu=psutil.cpu_percent()\n",
    "print ('Total CPUs utilized percentage :',vcpu,'%')\n",
    "finish=time.time()\n",
    "print('the duration of test is:',finish-start)\n",
    "print('Percentage of used RAM :',psutil.virtual_memory().percent,'%')\n",
    "\n",
    "train_scores_mean = cv_SVR.cv_results_[\"mean_train_score\"]\n",
    "test_scores_mean = cv_SVR.cv_results_[\"mean_test_score\"]\n",
    "print('The training score of the best estimator: ',train_scores_mean[np.argmax(test_scores_mean)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size = 0.2,random_state=17)\n",
    "xtrain, xval, ytrain, yval = train_test_split(xtrain, ytrain, test_size=0.25, random_state=7)\n",
    "\n",
    "X_stationary=xtest.filter(like='_nm_', axis=0)\n",
    "Y_stationary=ytest.filter(like='_nm_', axis=0)\n",
    "\n",
    "X_moving=xtest.drop(xtest.filter(like='_nm_', axis=0).index)\n",
    "Y_moving=ytest.drop(ytest.filter(like='_nm_', axis=0).index)\n",
    "\n",
    "X_pedestrians=X_moving.filter(regex='_s50-50_80_cli([0123]?[0-9]_)|_s100-0_|_s50-50_160_cli([01234567]?[0-9]_)',axis=0)\n",
    "Y_pedestrians=Y_moving.filter(regex='_s50-50_80_cli([0123]?[0-9]_)|_s100-0_|_s50-50_160_cli([01234567]?[0-9]_)',axis=0)\n",
    "\n",
    "X_vehicles=X_moving.filter(regex='_s50-50_80_cli(4[0-9]|[567][0-9]_)|_s0-100_|_s50-50_160_cli([89][0-9]|1[0-5][0-9])',axis=0)\n",
    "Y_vehicles=Y_moving.filter(regex='_s50-50_80_cli(4[0-9]|[567][0-9]_)|_s0-100_|_s50-50_160_cli([89][0-9]|1[0-5][0-9])',axis=0)\n",
    "\n",
    "\n",
    "#convert dataframe to tensor\n",
    "xtrain=torch.Tensor(xtrain.values)\n",
    "xtest=torch.Tensor(xtest.values)\n",
    "ytrain=torch.Tensor(ytrain.values).reshape(-1,1)\n",
    "ytest=torch.Tensor(ytest.values).reshape(-1,1)\n",
    "\n",
    "xval=torch.Tensor(xval.values)\n",
    "yval=torch.Tensor(yval.values).reshape(-1,1)\n",
    "\n",
    "X_stationary=torch.Tensor(X_stationary.values)\n",
    "Y_stationary=torch.Tensor(Y_stationary.values).reshape(-1,1)\n",
    "X_moving=torch.Tensor(X_moving.values)\n",
    "Y_moving=torch.Tensor(Y_moving.values).reshape(-1,1)\n",
    "X_pedestrians=torch.Tensor(X_pedestrians.values)\n",
    "Y_pedestrians=torch.Tensor(Y_pedestrians.values).reshape(-1,1)\n",
    "X_vehicles=torch.Tensor(X_vehicles.values)\n",
    "Y_vehicles=torch.Tensor(Y_vehicles.values).reshape(-1,1)\n",
    "\n",
    "print(xtest.shape,X_stationary.shape,X_moving.shape,X_pedestrians.shape,X_vehicles.shape)\n",
    "print(xtrain.shape, xtest.shape,xval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressorNet(torch.nn.Module):\n",
    "    def __init__(self, n_feature=10, n_hidden=10, n_output=1):\n",
    "        super(RegressorNet, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(n_feature, n_hidden)  # 95-(100-10)-1\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden1(x))\n",
    "        x = self.predict(x)  # no activation, aka Identity()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "EPOCH = 1500               # train the training data 500 times\n",
    "BATCH_SIZE = 300\n",
    "LR = 0.002\n",
    "\n",
    "torch_dataset=Data.TensorDataset(xtrain,ytrain)\n",
    "trainloader=Data.DataLoader(dataset=torch_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "start=time.time()\n",
    "net=RegressorNet(n_feature=maxnum_feature,n_hidden=50,n_output=1)\n",
    "opt=torch.optim.Adam(net.parameters(),lr=LR)\n",
    "loss_func=torch.nn.MSELoss()\n",
    "meanloss_train=[]\n",
    "meanloss_valid=[]\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "#     print('Epoch:',epoch)\n",
    "    step_loss=[]\n",
    "    for step,(batchx,batchy) in enumerate(trainloader):\n",
    "        output=net(batchx)\n",
    "        loss=loss_func(output,batchy)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        step_loss.append(loss.data.numpy().flatten()[0])\n",
    "#     print(step_loss)\n",
    "\n",
    "    meanloss_train.append(np.array(step_loss).mean())\n",
    "    valpre=net(xval)\n",
    "    val_loss=loss_func(valpre,yval)\n",
    "    meanloss_valid.append(val_loss.data.numpy().flatten()[0])\n",
    "    \n",
    "vcpu=psutil.cpu_percent()\n",
    "print ('Total CPUs utilized percentage :',vcpu,'%')\n",
    "finish=time.time()  \n",
    "print('the duration of training is:',finish-start)\n",
    "print('Percentage of used RAM :',psutil.virtual_memory().percent,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the MSE score for different type in the test dataset by applying the same model\n",
    "print('----------------NN---------------')\n",
    "print('All: ',loss_func(net(xtest),ytest).data.numpy())\n",
    "print('stationary: ',loss_func(net(X_stationary),Y_stationary).data.numpy())\n",
    "print('moving: ',loss_func(net(X_moving),Y_moving).data.numpy())\n",
    "print('pedestrians: ',loss_func(net(X_pedestrians),Y_pedestrians).data.numpy())\n",
    "print('vehicles: ',loss_func(net(X_vehicles),Y_vehicles).data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "torch.save(net.state_dict(), 'model_%s_NN_MSE.pt'%mark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
